{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries for overlapping 5-second chunks with percentiles have been saved to ./chunk_summaries1.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './archive-2/02-14-2018.csv'\n",
    "\n",
    "# Function to calculate metrics for a chunk\n",
    "def calculate_forward_only_metrics(chunk, all_flow_durations):\n",
    "    metrics = {}\n",
    "    total_flow_duration = chunk['Flow Duration'].sum()\n",
    "    metrics['Total_Flow_Duration_Percentile'] = percentileofscore(all_flow_durations, total_flow_duration)  # Percentile rank\n",
    "    metrics['Avg_Tot_Fwd_Pkts'] = chunk['Tot Fwd Pkts'].mean()  # Average forward packets\n",
    "    metrics['Total_Hits_All_Ports'] = chunk['Dst Port'].value_counts().sum()  # Total hits across all ports\n",
    "    metrics['Unique_Ports'] = chunk['Dst Port'].nunique()  # Number of unique ports\n",
    "    metrics['Port_Hit_Variance'] = chunk['Dst Port'].value_counts().var()  # Variance in hits across ports\n",
    "    return metrics\n",
    "\n",
    "# Function to generate a summary from updated metrics\n",
    "def generate_summary(metrics):\n",
    "    text = (\n",
    "        f\"Total flow duration percentile {metrics['Total_Flow_Duration_Percentile']:.2f}, \"\n",
    "        f\"avg forward packets {metrics['Avg_Tot_Fwd_Pkts']:.2f}, \"\n",
    "        f\"total hits across all ports {metrics['Total_Hits_All_Ports']}, \"\n",
    "        f\"{metrics['Unique_Ports']} unique ports active, \"\n",
    "       \n",
    "    )\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    # Step 1: Load and preprocess the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert Timestamp column to datetime\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce', format='%d/%m/%Y %H:%M:%S')\n",
    "    data = data.dropna(subset=['Timestamp'])  # Remove rows with invalid Timestamps\n",
    "\n",
    "    # Filter out rows where the label isn't \"Benign\"\n",
    "    if 'Label' in data.columns:\n",
    "        data = data[data['Label'] == 'Benign']\n",
    "    else:\n",
    "        raise ValueError(\"The dataset does not contain a 'Label' column. Cannot filter for Benign data.\")\n",
    "\n",
    "    # Exclude rows with zero byte values\n",
    "    data = data[(data['TotLen Fwd Pkts'] > 0) | (data['TotLen Bwd Pkts'] > 0)]\n",
    "\n",
    "    # Create time-based chunks (5 seconds with 50% overlap)\n",
    "    data['Timestamp_Seconds'] = (data['Timestamp'].astype('int64') // 1e9).astype(int)\n",
    "    chunk_start = (data['Timestamp_Seconds'] // 5) * 5\n",
    "    data['Chunk Start'] = chunk_start\n",
    "\n",
    "    # Extract all total flow durations for percentile computation\n",
    "    all_total_flow_durations = [\n",
    "        data[data['Chunk Start'] == start]['Flow Duration'].sum()\n",
    "        for start in data['Chunk Start'].unique()\n",
    "    ]\n",
    "\n",
    "    # Step 2: Process overlapping 5-second chunks\n",
    "    chunk_results = []\n",
    "    chunk_ids = data['Chunk Start'].unique()\n",
    "\n",
    "    for start in chunk_ids:\n",
    "        chunk_data = data[data['Chunk Start'] == start]\n",
    "        if not chunk_data.empty:\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_forward_only_metrics(chunk_data, all_total_flow_durations)\n",
    "            metrics['Chunk_ID'] = start  # Add chunk identifier for reference\n",
    "            \n",
    "            # Generate summary text\n",
    "            summary_text = generate_summary(metrics)\n",
    "            chunk_results.append({'Chunk ID': start, 'Summary': summary_text})\n",
    "\n",
    "    # Step 3: Convert results to a DataFrame\n",
    "    chunk_summary_df = pd.DataFrame(chunk_results)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    output_file = './chunk_summaries1.csv'\n",
    "    chunk_summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Summaries for overlapping 5-second chunks with percentiles have been saved to {output_file}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries for overlapping 5-second chunks with percentiles have been saved to ./chunk_summaries2.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './archive-2/02-15-2018.csv'\n",
    "\n",
    "# Function to calculate metrics for a chunk\n",
    "def calculate_forward_only_metrics(chunk, all_flow_durations):\n",
    "    metrics = {}\n",
    "    total_flow_duration = chunk['Flow Duration'].sum()\n",
    "    metrics['Total_Flow_Duration_Percentile'] = percentileofscore(all_flow_durations, total_flow_duration)  # Percentile rank\n",
    "    metrics['Avg_Tot_Fwd_Pkts'] = chunk['Tot Fwd Pkts'].mean()  # Average forward packets\n",
    "    metrics['Total_Hits_All_Ports'] = chunk['Dst Port'].value_counts().sum()  # Total hits across all ports\n",
    "    metrics['Unique_Ports'] = chunk['Dst Port'].nunique()  # Number of unique ports\n",
    "    metrics['Port_Hit_Variance'] = chunk['Dst Port'].value_counts().var()  # Variance in hits across ports\n",
    "    return metrics\n",
    "\n",
    "# Function to generate a summary from updated metrics\n",
    "def generate_summary(metrics):\n",
    "    text = (\n",
    "        f\"Total flow duration percentile {metrics['Total_Flow_Duration_Percentile']:.2f}, \"\n",
    "        f\"avg forward packets {metrics['Avg_Tot_Fwd_Pkts']:.2f}, \"\n",
    "        f\"total hits across all ports {metrics['Total_Hits_All_Ports']}, \"\n",
    "        f\"{metrics['Unique_Ports']} unique ports active, \"\n",
    "        \n",
    "    )\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    # Step 1: Load and preprocess the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert Timestamp column to datetime\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce', format='%d/%m/%Y %H:%M:%S')\n",
    "    data = data.dropna(subset=['Timestamp'])  # Remove rows with invalid Timestamps\n",
    "\n",
    "    # Filter out rows where the label isn't \"Benign\"\n",
    "    if 'Label' in data.columns:\n",
    "        data = data[data['Label'] == 'Benign']\n",
    "    else:\n",
    "        raise ValueError(\"The dataset does not contain a 'Label' column. Cannot filter for Benign data.\")\n",
    "\n",
    "    # Exclude rows with zero byte values\n",
    "    data = data[(data['TotLen Fwd Pkts'] > 0) | (data['TotLen Bwd Pkts'] > 0)]\n",
    "\n",
    "    # Create time-based chunks (5 seconds with 50% overlap)\n",
    "    data['Timestamp_Seconds'] = (data['Timestamp'].astype('int64') // 1e9).astype(int)\n",
    "    chunk_start = (data['Timestamp_Seconds'] // 5) * 5\n",
    "    data['Chunk Start'] = chunk_start\n",
    "\n",
    "    # Extract all total flow durations for percentile computation\n",
    "    all_total_flow_durations = [\n",
    "        data[data['Chunk Start'] == start]['Flow Duration'].sum()\n",
    "        for start in data['Chunk Start'].unique()\n",
    "    ]\n",
    "\n",
    "    # Step 2: Process overlapping 5-second chunks\n",
    "    chunk_results = []\n",
    "    chunk_ids = data['Chunk Start'].unique()\n",
    "\n",
    "    for start in chunk_ids:\n",
    "        chunk_data = data[data['Chunk Start'] == start]\n",
    "        if not chunk_data.empty:\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_forward_only_metrics(chunk_data, all_total_flow_durations)\n",
    "            metrics['Chunk_ID'] = start  # Add chunk identifier for reference\n",
    "            \n",
    "            # Generate summary text\n",
    "            summary_text = generate_summary(metrics)\n",
    "            chunk_results.append({'Chunk ID': start, 'Summary': summary_text})\n",
    "\n",
    "    # Step 3: Convert results to a DataFrame\n",
    "    chunk_summary_df = pd.DataFrame(chunk_results)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    output_file = './chunk_summaries2.csv'\n",
    "    chunk_summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Summaries for overlapping 5-second chunks with percentiles have been saved to {output_file}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries for overlapping 5-second chunks with percentiles have been saved to ./chunk_summaries3.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './archive-2/02-22-2018.csv'\n",
    "\n",
    "# Function to calculate metrics for a chunk\n",
    "def calculate_forward_only_metrics(chunk, all_flow_durations):\n",
    "    metrics = {}\n",
    "    total_flow_duration = chunk['Flow Duration'].sum()\n",
    "    metrics['Total_Flow_Duration_Percentile'] = percentileofscore(all_flow_durations, total_flow_duration)  # Percentile rank\n",
    "    metrics['Avg_Tot_Fwd_Pkts'] = chunk['Tot Fwd Pkts'].mean()  # Average forward packets\n",
    "    metrics['Total_Hits_All_Ports'] = chunk['Dst Port'].value_counts().sum()  # Total hits across all ports\n",
    "    metrics['Unique_Ports'] = chunk['Dst Port'].nunique()  # Number of unique ports\n",
    "    metrics['Port_Hit_Variance'] = chunk['Dst Port'].value_counts().var()  # Variance in hits across ports\n",
    "    return metrics\n",
    "\n",
    "# Function to generate a summary from updated metrics\n",
    "def generate_summary(metrics):\n",
    "    text = (\n",
    "        f\"Total flow duration percentile {metrics['Total_Flow_Duration_Percentile']:.2f}, \"\n",
    "        f\"avg forward packets {metrics['Avg_Tot_Fwd_Pkts']:.2f}, \"\n",
    "        f\"total hits across all ports {metrics['Total_Hits_All_Ports']}, \"\n",
    "        f\"{metrics['Unique_Ports']} unique ports active, \"\n",
    "       \n",
    "    )\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    # Step 1: Load and preprocess the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert Timestamp column to datetime\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce', format='%d/%m/%Y %H:%M:%S')\n",
    "    data = data.dropna(subset=['Timestamp'])  # Remove rows with invalid Timestamps\n",
    "\n",
    "    # Filter out rows where the label isn't \"Benign\"\n",
    "    if 'Label' in data.columns:\n",
    "        data = data[data['Label'] == 'Benign']\n",
    "    else:\n",
    "        raise ValueError(\"The dataset does not contain a 'Label' column. Cannot filter for Benign data.\")\n",
    "\n",
    "    # Exclude rows with zero byte values\n",
    "    data = data[(data['TotLen Fwd Pkts'] > 0) | (data['TotLen Bwd Pkts'] > 0)]\n",
    "\n",
    "    # Create time-based chunks (5 seconds with 50% overlap)\n",
    "    data['Timestamp_Seconds'] = (data['Timestamp'].astype('int64') // 1e9).astype(int)\n",
    "    chunk_start = (data['Timestamp_Seconds'] // 5) * 5\n",
    "    data['Chunk Start'] = chunk_start\n",
    "\n",
    "    # Extract all total flow durations for percentile computation\n",
    "    all_total_flow_durations = [\n",
    "        data[data['Chunk Start'] == start]['Flow Duration'].sum()\n",
    "        for start in data['Chunk Start'].unique()\n",
    "    ]\n",
    "\n",
    "    # Step 2: Process overlapping 5-second chunks\n",
    "    chunk_results = []\n",
    "    chunk_ids = data['Chunk Start'].unique()\n",
    "\n",
    "    for start in chunk_ids:\n",
    "        chunk_data = data[data['Chunk Start'] == start]\n",
    "        if not chunk_data.empty:\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_forward_only_metrics(chunk_data, all_total_flow_durations)\n",
    "            metrics['Chunk_ID'] = start  # Add chunk identifier for reference\n",
    "            \n",
    "            # Generate summary text\n",
    "            summary_text = generate_summary(metrics)\n",
    "            chunk_results.append({'Chunk ID': start, 'Summary': summary_text})\n",
    "\n",
    "    # Step 3: Convert results to a DataFrame\n",
    "    chunk_summary_df = pd.DataFrame(chunk_results)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    output_file = './chunk_summaries3.csv'\n",
    "    chunk_summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Summaries for overlapping 5-second chunks with percentiles have been saved to {output_file}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries for overlapping 5-second chunks with percentiles have been saved to ./chunk_summaries4.csv.\n"
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.stats import percentileofscore\n",
    "\n",
    "    # Load the dataset\n",
    "    file_path = './archive-2/02-23-2018.csv'\n",
    "\n",
    "    # Function to calculate metrics for a chunk\n",
    "    def calculate_forward_only_metrics(chunk, all_flow_durations):\n",
    "        metrics = {}\n",
    "        total_flow_duration = chunk['Flow Duration'].sum()\n",
    "        metrics['Total_Flow_Duration_Percentile'] = percentileofscore(all_flow_durations, total_flow_duration)  # Percentile rank\n",
    "        metrics['Avg_Tot_Fwd_Pkts'] = chunk['Tot Fwd Pkts'].mean()  # Average forward packets\n",
    "        metrics['Total_Hits_All_Ports'] = chunk['Dst Port'].value_counts().sum()  # Total hits across all ports\n",
    "        metrics['Unique_Ports'] = chunk['Dst Port'].nunique()  # Number of unique ports\n",
    "        metrics['Port_Hit_Variance'] = chunk['Dst Port'].value_counts().var()  # Variance in hits across ports\n",
    "        return metrics\n",
    "\n",
    "    # Function to generate a summary from updated metrics\n",
    "    def generate_summary(metrics):\n",
    "        text = (\n",
    "            f\"Total flow duration percentile {metrics['Total_Flow_Duration_Percentile']:.2f}, \"\n",
    "            f\"avg forward packets {metrics['Avg_Tot_Fwd_Pkts']:.2f}, \"\n",
    "            f\"total hits across all ports {metrics['Total_Hits_All_Ports']}, \"\n",
    "            f\"{metrics['Unique_Ports']} unique ports active, \"\n",
    "            \n",
    "        )\n",
    "        return text\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load and preprocess the data\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert Timestamp column to datetime\n",
    "        data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce', format='%d/%m/%Y %H:%M:%S')\n",
    "        data = data.dropna(subset=['Timestamp'])  # Remove rows with invalid Timestamps\n",
    "\n",
    "        # Filter out rows where the label isn't \"Benign\"\n",
    "        if 'Label' in data.columns:\n",
    "            data = data[data['Label'] == 'Benign']\n",
    "        else:\n",
    "            raise ValueError(\"The dataset does not contain a 'Label' column. Cannot filter for Benign data.\")\n",
    "\n",
    "        # Exclude rows with zero byte values\n",
    "        data = data[(data['TotLen Fwd Pkts'] > 0) | (data['TotLen Bwd Pkts'] > 0)]\n",
    "\n",
    "        # Create time-based chunks (5 seconds with 50% overlap)\n",
    "        data['Timestamp_Seconds'] = (data['Timestamp'].astype('int64') // 1e9).astype(int)\n",
    "        chunk_start = (data['Timestamp_Seconds'] // 5) * 5\n",
    "        data['Chunk Start'] = chunk_start\n",
    "\n",
    "        # Extract all total flow durations for percentile computation\n",
    "        all_total_flow_durations = [\n",
    "            data[data['Chunk Start'] == start]['Flow Duration'].sum()\n",
    "            for start in data['Chunk Start'].unique()\n",
    "        ]\n",
    "\n",
    "        # Step 2: Process overlapping 5-second chunks\n",
    "        chunk_results = []\n",
    "        chunk_ids = data['Chunk Start'].unique()\n",
    "\n",
    "        for start in chunk_ids:\n",
    "            chunk_data = data[data['Chunk Start'] == start]\n",
    "            if not chunk_data.empty:\n",
    "                # Calculate metrics\n",
    "                metrics = calculate_forward_only_metrics(chunk_data, all_total_flow_durations)\n",
    "                metrics['Chunk_ID'] = start  # Add chunk identifier for reference\n",
    "                \n",
    "                # Generate summary text\n",
    "                summary_text = generate_summary(metrics)\n",
    "                chunk_results.append({'Chunk ID': start, 'Summary': summary_text})\n",
    "\n",
    "        # Step 3: Convert results to a DataFrame\n",
    "        chunk_summary_df = pd.DataFrame(chunk_results)\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        output_file = './chunk_summaries4.csv'\n",
    "        chunk_summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Summaries for overlapping 5-second chunks with percentiles have been saved to {output_file}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarcanning-thompson/mdm3_phaseB/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciBERT model and tokenizer saved to ./sciBERT_model_uncased.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"  # Or \"allenai/scibert_scivocab_cased\"\n",
    "\n",
    "# Load and download the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "save_path = \"./sciBERT_model_uncased\"  # Specify the save path\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(f\"SciBERT model and tokenizer saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries converted to embeddings and saved to ./Train_ddos4.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Path to the downloaded SciBERT model\n",
    "model_path = \"./sciBERT_model_uncased\"  # Replace with your actual path\n",
    "\n",
    "# Load the locally saved tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Load the CSV file containing summaries\n",
    "input_file = './chunk_summaries4.csv'\n",
    "output_file = './Train_ddos4.csv'\n",
    "summary_df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to generate embeddings for summaries\n",
    "def convert_to_embeddings(summary_df):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for summary in summary_df['Summary']:\n",
    "        # Tokenize the summary\n",
    "        inputs = tokenizer(summary, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Generate embeddings using SciBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Use the [CLS] token embedding (first token) as the sentence representation\n",
    "        sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0).numpy()\n",
    "\n",
    "        # Append the embedding to the list\n",
    "        embeddings_list.append(sentence_embedding)\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings_normalized = normalize(embeddings_list, axis=1)\n",
    "\n",
    "    # Add normalized embeddings as a new column\n",
    "    summary_df['Embeddings'] = [embedding.tolist() for embedding in embeddings_normalized]\n",
    "    return summary_df\n",
    "\n",
    "# Apply the embedding generation function\n",
    "summary_df = convert_to_embeddings(summary_df)\n",
    "\n",
    "# Save the updated DataFrame with embeddings\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Summaries converted to embeddings and saved to {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
